{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Stemming vs lemmatization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "source": [
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "porter_stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#list of tokenized words\n",
    "words = ['cared','university','mice','easily','singing',\n",
    "\t'language','corpora','singer','sportingly','rocks']\n",
    "\n",
    "#stem's of each word\n",
    "stem_words_snowball = []\n",
    "stem_words_porter = []\n",
    "stem_words_lemma = []"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "  xs = snow_stemmer.stem(w)\n",
    "  xp = porter_stemmer.stem(w)\n",
    "  xl = lemmatizer.lemmatize(w)\n",
    "  \n",
    "  stem_words_snowball.append(xs)\n",
    "  stem_words_porter.append(xp)\n",
    "  stem_words_lemma.append(xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "word           snow_ball      porter         lemma          \n------------------------------------------------------------\ncared          care           care           cared          \nuniversity     univers        univers        university     \nmice           mice           mice           mouse          \neasily         easili         easili         easily         \nsinging        sing           sing           singing        \nlanguage       languag        languag        language       \ncorpora        corpora        corpora        corpus         \nsinger         singer         singer         singer         \nsportingly     sport          sportingli     sportingly     \nrocks          rock           rock           rock           \n"
     ]
    }
   ],
   "source": [
    "print(f\"{'word':15}{'snow_ball':15}{'porter':15}{'lemma':15}\")\n",
    "print('-'*60)\n",
    "for word,snow_ball,porter,lemma in zip(words,stem_words_snowball,stem_words_porter,stem_words_lemma):\n",
    "  print(f\"{word:15}{snow_ball:15}{porter:15}{lemma:15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Hi Machine learning aspirants.', 'Hope you are doing great.', 'NLP is good.'] ['Hi', 'Machine', 'learning', 'aspirants', '.', 'Hope', 'you', 'are', 'doing', 'great', '.', 'NLP', 'is', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "corpus = 'Hi Machine learning aspirants. Hope you are doing great. NLP is good.'\n",
    "\n",
    "sent_tokens = nltk.sent_tokenize(corpus)\n",
    "word_tokens = nltk.word_tokenize(corpus)\n",
    "\n",
    "print(sent_tokens,word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['believe god believe', 'greatest sin think weak', 'believe world feet']\n['believe', 'feet', 'god', 'greatest', 'sin', 'think', 'weak', 'world']\n{'believe': 3, 'god': 1, 'greatest': 1, 'sin': 1, 'think': 1, 'weak': 1, 'world': 1, 'feet': 1}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "corpus = \"You cannot believe in god until you believe in yourself. The greatest sin is to think that you are weak. Believe in yourself and the world will be at your feet.\" #quotes of swami vivekananda\n",
    "pattern = r'[^a-zA-Z\\s]'\n",
    "\n",
    "corpus = nltk.sent_tokenize(corpus)      #converting paragraph into sentences\n",
    "\n",
    "corpus_cleaned = []\n",
    "words = []\n",
    "for sentence in corpus:\n",
    "    sent_cleaned = re.sub(pattern,'',sentence)\n",
    "    sent = ''\n",
    "    word_tokens = nltk.word_tokenize(sent_cleaned)\n",
    "    words_filtered = [word.lower() for word in word_tokens if not word.lower() in stop_words]\n",
    "    sent = ' '.join(words_filtered)\n",
    "    corpus_cleaned.append(sent)\n",
    "    words.extend(words_filtered)\n",
    "\n",
    "words_set = sorted(set(words))\n",
    "\n",
    "print(corpus_cleaned)\n",
    "print(words_set)\n",
    "\n",
    "bow = dict([(a,words.count(a)) for a in words])\n",
    "print(bow)"
   ]
  },
  {
   "source": [
    "### Bag of Words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'believe god believe': [3, 0, 1, 0, 0, 0, 0, 0], 'greatest sin think weak': [0, 0, 0, 1, 1, 1, 1, 0], 'believe world feet': [3, 1, 0, 0, 0, 0, 0, 1], 'bag of words': ['believe', 'feet', 'god', 'greatest', 'sin', 'think', 'weak', 'world']}\n"
     ]
    }
   ],
   "source": [
    "vectors = {}.fromkeys(corpus_cleaned, [])\n",
    "for sentence in vectors:\n",
    "    vector = []\n",
    "    for word in words_set:\n",
    "        if word in sentence:\n",
    "            vector.append(bow[word])\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    vectors[sentence] = vector\n",
    "vectors['bag of words']= words_set\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}