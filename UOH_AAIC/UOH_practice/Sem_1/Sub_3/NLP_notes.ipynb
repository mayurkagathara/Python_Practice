{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Stemming vs lemmatization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "source": [
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "porter_stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#list of tokenized words\n",
    "words = ['cared','university','mice','easily','singing',\n",
    "\t'language','corpora','singer','sportingly','rocks']\n",
    "\n",
    "#stem's of each word\n",
    "stem_words_snowball = []\n",
    "stem_words_porter = []\n",
    "stem_words_lemma = []"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "  xs = snow_stemmer.stem(w)\n",
    "  xp = porter_stemmer.stem(w)\n",
    "  xl = lemmatizer.lemmatize(w)\n",
    "  \n",
    "  stem_words_snowball.append(xs)\n",
    "  stem_words_porter.append(xp)\n",
    "  stem_words_lemma.append(xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "word           snow_ball      porter         lemma          \n------------------------------------------------------------\ncared          care           care           cared          \nuniversity     univers        univers        university     \nmice           mice           mice           mouse          \neasily         easili         easili         easily         \nsinging        sing           sing           singing        \nlanguage       languag        languag        language       \ncorpora        corpora        corpora        corpus         \nsinger         singer         singer         singer         \nsportingly     sport          sportingli     sportingly     \nrocks          rock           rock           rock           \n"
     ]
    }
   ],
   "source": [
    "print(f\"{'word':15}{'snow_ball':15}{'porter':15}{'lemma':15}\")\n",
    "print('-'*60)\n",
    "for word,snow_ball,porter,lemma in zip(words,stem_words_snowball,stem_words_porter,stem_words_lemma):\n",
    "  print(f\"{word:15}{snow_ball:15}{porter:15}{lemma:15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Hi Machine learning aspirants.', 'Hope you are doing great.', 'NLP is good.'] ['Hi', 'Machine', 'learning', 'aspirants', '.', 'Hope', 'you', 'are', 'doing', 'great', '.', 'NLP', 'is', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "corpus = 'Hi Machine learning aspirants. Hope you are doing great. NLP is good.'\n",
    "\n",
    "sent_tokens = nltk.sent_tokenize(corpus)\n",
    "word_tokens = nltk.word_tokenize(corpus)\n",
    "\n",
    "print(sent_tokens,word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['You cannot believe in god until you believe in yourself', 'The greatest sin is to think that you are weak', 'Believe in yourself and the world will be at your feet']\n['believe', 'god', 'believe', 'greatest', 'sin', 'think', 'weak', 'believe', 'world', 'feet']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "corpus = \"You cannot believe in god until you believe in yourself. The greatest sin is to think that you are weak. Believe in yourself and the world will be at your feet.\" #quotes of swami vivekananda\n",
    "pattern = r'[^a-zA-Z\\s]'\n",
    "\n",
    "sent_tokens = nltk.sent_tokenize(corpus)\n",
    "corpus_cleaned = re.sub(pattern,'',corpus)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = nltk.word_tokenize(corpus_cleaned)\n",
    "words_filtered = [word.lower() for word in word_tokens if not word.lower() in stop_words]\n",
    "sent_cleaned = [re.sub(pattern,'',sent) for sent in sent_tokens]\n",
    "\n",
    "print(sent_cleaned)\n",
    "print(words_filtered)\n",
    "#bag of words\n"
   ]
  },
  {
   "source": [
    "### Bag of Words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['You cannot believe in god until you believe in yourself',\n",
       " 'The greatest sin is to think that you are weak',\n",
       " 'Believe in yourself and the world will be at your feet']"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "sent_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mayur/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}