{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn import impute\n",
    "## OR ##\n",
    "#from sklearn.impute import SimpleImputer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit Learn basic functions\n",
    "**Utilities**\n",
    "* 1.0 Data Gathering\n",
    "* 2.0 Preprocessing\n",
    "* 3.0 Model Selection\n",
    "* 4.0 Classification\n",
    "* 5.0 Regression\n",
    "* 6.0 Clustering\n",
    "* 7.0 Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Data Gathering\n",
    "\n",
    "**1.1 Gathering data from web using pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 32)\n",
      "(569, 30)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(569, 30)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "cancer_set = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data',                          \n",
    "                        header = None)\n",
    "print(cancer_set.shape)\n",
    "# print(cancer_set.head())\n",
    "# print(cancer_set.info())\n",
    "# print(cancer_set.describe())\n",
    "cancer_features = cancer_set.iloc[:,2:]  #iloc[all rows,columns from 2 to all]\n",
    "\n",
    "print(cancer_features.shape)\n",
    "print(type(cancer_features))\n",
    "\n",
    "cancer_features = cancer_features.values #converting data frame to numpy array\n",
    "print(cancer_features.shape)\n",
    "print(type(cancer_features))\n",
    "\n",
    "cancer_features_names = ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', \n",
    "                         'mean compactness', 'mean concavity','mean concave points', 'mean symmetry',\n",
    "                         'mean fractal dimension','radius error','texture error','perimeter error',\n",
    "                         'area error', 'smoothness error','compactness error','concavity error',\n",
    "                         'concave points error','symmetry error','fractal dimension error','worst radius',\n",
    "                         'worst texture', 'worst perimeter', 'worst area','worst smoothness', 'worst compactness',\n",
    "                         'worst concavity','worst concave points','worst symmetry','worst fractal dimension']\n",
    "\n",
    "cancer_target = cancer_set.iloc[:, 1] #melign and benign column extract\n",
    "\n",
    "# Replacing 'M' with 0 and 'B' with 1\n",
    "cancer_target = cancer_target.replace(['M', 'B'], [0, 1])\n",
    "\n",
    "# Converting to numpy array\n",
    "cancer_target = cancer_target.values #convert panda to numpy\n",
    "\n",
    "print(type(cancer_target))\n",
    "print(cancer_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 Gathering data from scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "# import sklearn.datasets as datasets\n",
    "\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "\n",
    "print(breast_cancer.data.shape)\n",
    "print(breast_cancer.target.shape)\n",
    "\n",
    "# this is fully processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Preprocessing\n",
    "- 2.1 Standardization mean removal  \n",
    "- 2.2 Scaling  \n",
    "- 2.3 Normalization  \n",
    "- 2.4 Binarization  \n",
    "- 2.5 One Hot Encoding  \n",
    "- 2.6 Label Encoding  \n",
    "- 2.7 Imputation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Standardization  \n",
    "Standardization or Mean Removal is the process of transforming each feature vector into a normal distribution with **mean 0 and variance 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of each feature after Standardization :\n",
      "\n",
      "\n",
      "[-3.16286735e-15 -6.53060890e-15 -7.07889127e-16 -8.79983452e-16\n",
      "  6.13217737e-15 -1.12036918e-15 -4.42138027e-16  9.73249991e-16\n",
      " -1.97167024e-15 -1.45363120e-15 -9.07641468e-16 -8.85349205e-16\n",
      "  1.77367396e-15 -8.29155139e-16 -7.54180940e-16 -3.92187747e-16\n",
      "  7.91789988e-16 -2.73946068e-16 -3.10823423e-16 -3.36676596e-16\n",
      " -2.33322442e-15  1.76367415e-15 -1.19802625e-15  5.04966114e-16\n",
      " -5.21317026e-15 -2.17478837e-15  6.85645643e-16 -1.41265636e-16\n",
      " -2.28956670e-15  2.57517109e-15]\n",
      "\n",
      "Std. of each feature after Standardization :\n",
      "\n",
      "\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# import sklearn.preprocessing as preprocessing\n",
    "standardizer = preprocessing.StandardScaler()\n",
    "standardizer = standardizer.fit(breast_cancer.data)  # this takes only numbers not string\n",
    "breast_cancer_standardized = standardizer.transform(breast_cancer.data)\n",
    "\n",
    "print('Mean of each feature after Standardization :\\n\\n')\n",
    "print(breast_cancer_standardized.mean(axis=0))\n",
    "print('\\nStd. of each feature after Standardization :\\n\\n')\n",
    "print(breast_cancer_standardized.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Scaling\n",
    "*Scaling transforms existing data values to lie between a minimum and maximum value.*\n",
    "\n",
    "**MinMaxScaler** transforms data to range **0 and 1**.  \n",
    "**MaxAbsScaler** transforms data to range **-1 and 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mimmax scaler to fit data in range 1 to 10\n",
    "\n",
    "# min max scaler for default range 0 to 1\n",
    "# min_max_scaler = preprocessing.MinMaxScaler().fit(breast_cancer.data)\n",
    "# breast_cancer_minmaxscaled = min_max_scaler.transform(breast_cancer.data)\n",
    "# feature range changed to 0 to 10\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 10)).fit(breast_cancer.data)\n",
    "breast_cancer_minmaxscaled10 = min_max_scaler.transform(breast_cancer.data)\n",
    "\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler().fit(breast_cancer.data)\n",
    "breast_cancer_maxabsscaled = max_abs_scaler.transform(breast_cancer.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Normalization\n",
    "*Normalization scales each sample to have a unit norm.*  \n",
    "Normalization can be achieved with **'l1', 'l2', and 'max'** norms.  \n",
    "'l1' norm makes the **sum of absolute values of each row as 1**, and 'l2' norm makes the **sum of squares of each row as 1**.  \n",
    "'l1' norm is *insensitive to outliers*.  \n",
    "By default l2 norm is considered. Hence, *removing outliers is recommended before applying l2 norm.*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = preprocessing.Normalizer(norm='l1').fit(breast_cancer.data)\n",
    "breast_cancer_normalized = normalizer.transform(breast_cancer.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Binarization\n",
    "*Binarization is the process of transforming data points to 0 or 1 based on a given threshold.*  \n",
    "\n",
    "Any value **above the threshold** is transformed to **1**, and any value **below the threshold** is transformed to **0**.  \n",
    "***By default, a threshold of 0 is used.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "binarizer = preprocessing.Binarizer(threshold=3.0).fit(breast_cancer.data)\n",
    "breast_cancer_binarized = binarizer.transform(breast_cancer.data)\n",
    "print(breast_cancer_binarized[:5,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 OneHotEncoder\n",
    "*OneHotEncoder converts categorical integer values into one-hot vectors.*  \n",
    "In an one-hot vector, **every category** is transformed into a **binary attribute having only 0 and 1 values.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]]\n",
      "[[0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# An example creating two binary attributes for the categorical integers 1 and 2\n",
    "onehotencoder = preprocessing.OneHotEncoder()\n",
    "onehotencoder = onehotencoder.fit([[1], [1], [1], [2], [2], [1]])\n",
    "# Transforming category values 1 and 2 to one-hot vectors\n",
    "print(onehotencoder.transform([[1]]).toarray())\n",
    "print(onehotencoder.transform([[2]]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Label Encoding\n",
    "*Label Encoding is a step in which, in which categorical features are represented as categorical integers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of transforming categorical values [\"benign\",\"malignant\"]into[0, 1] is shown below.\n",
    "\n",
    "labels = ['malignant', 'benign', 'malignant', 'benign']\n",
    "labelencoder = preprocessing.LabelEncoder()\n",
    "labelencoder = labelencoder.fit(labels)\n",
    "\n",
    "bc_labelencoded = labelencoder.transform(breast_cancer.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Imputation\n",
    "*Imputation replaces missing values with either median, mean, or the most common value of the column or row in which the missing values exist.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below example replaces missing values, represented by np.nan, with the mean of respective column (axis 0).\n",
    "# \n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "imputer = imputer.fit(breast_cancer.data)\n",
    "breast_cancer_imputed = imputer.transform(breast_cancer.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In fresco play model selction and other topics are not covered. It now covers some ML techniques**\n",
    "- Nearest Neighbour technique\n",
    "- Decision Tree technique\n",
    "- Esemble Method\n",
    "- Support Vector Machine\n",
    "- Clustering technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbour technique\n",
    "*Nearest neighbors method is used to determine a predefined number of data points that are closer to a sample point and predict its label.*\n",
    "\n",
    "- **sklearn.neighbors** provides utilities for unsupervised and supervised neighbors-based learning methods.\n",
    "- scikit-learn implements two different nearest neighbors classifiers:\n",
    "  - *KNeighborsClassifier*   \n",
    "  classifies based on k nearest neighbors of every query point, where k is an integer value specified by the user\n",
    "  - *RadiusNeighborsClassifier*   \n",
    "  classifies based on the number of neighbors present in a fixed radius r of every training point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Train Data : 0.9460093896713615\n",
      "Accuracy of Test Data : 0.9300699300699301\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "cancer = datasets.load_breast_cancer()  # Loading the data set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cancer.data, cancer.target,\n",
    "                                                    stratify=cancer.target,random_state=42)\n",
    "knn_classifier = KNeighborsClassifier()   \n",
    "knn_classifier = knn_classifier.fit(X_train, Y_train)\n",
    "\n",
    "print('Accuracy of Train Data :', knn_classifier.score(X_train,Y_train))\n",
    "print('Accuracy of Test Data :', knn_classifier.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "*Decision Trees is another Supervised Learning method used for **Classification** **and Regression**.*\n",
    "\n",
    "- Decision Trees learn simple decision rules from training data and build a Model.\n",
    "- **DecisionTreeClassifier** and **DecisionTreeRegressor** are the two utilities from **sklearn.tree**, which can be used for classification and regression respectively.\n",
    "\n",
    "### Advantages\n",
    "- Decision Trees are easy to understand.\n",
    "- They often do not require any preprocessing.\n",
    "- Decision Trees can learn from both numerical and categorical data.\n",
    "\n",
    "### Disadvantages\n",
    "- Decision trees sometimes become complex, which do not generalize well and leads to overfitting.\n",
    "    - Overfitting can be addressed by placing the least number of samples needed at a leaf node or placing the highest depth of the tree.\n",
    "\n",
    "- A small variation in data can result in a completely different tree.\n",
    "    - This problem can be addressed by using decision trees within an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Train Data : 1.0\n",
      "Accuracy of Test Data : 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "dt_classifier = DecisionTreeClassifier()   \n",
    "dt_classifier = dt_classifier.fit(X_train, Y_train) \n",
    "\n",
    "print('Accuracy of Train Data :', dt_classifier.score(X_train,Y_train))\n",
    "print('Accuracy of Test Data :', dt_classifier.score(X_test,Y_test))\n",
    "\n",
    "# This is overfitting- Train data accurancy is 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finetune the Model, by changing max depth value to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Train Data : 0.9460093896713615\n",
      "Accuracy of Test Data : 0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = DecisionTreeClassifier(max_depth=2)   \n",
    "dt_classifier = dt_classifier.fit(X_train, Y_train) \n",
    "\n",
    "print('Accuracy of Train Data :', dt_classifier.score(X_train,Y_train))\n",
    "print('Accuracy of Test Data :', dt_classifier.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods\n",
    "*Ensemble methods combine predictions of other learning algorithms, to improve the generalization.*\n",
    "\n",
    "Ensemble methods are two types:\n",
    "- **Averaging Methods**: They build several base estimators independently and finally average their predictions.\n",
    "    - E.g.: ***Bagging Methods, Forests of randomised trees***\n",
    "- **Boosting Methods**: They build base estimators sequentially and try to reduce the bias of the combined estimator.\n",
    "    - E.g.: ***Adaboost, Gradient Tree Boosting***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Bagging Methods\n",
    "**Bagging Methods** draw random subsets of the original dataset, build an estimator and aggregate individual results to form a final one.\n",
    "\n",
    "- **BaggingClassifier** and **BaggingRegressor** are the utilities from **sklearn.ensemble** to deal with Bagging.\n",
    "\n",
    "#### 2. Randomized Trees\n",
    "**sklearn.ensemble** offers two types of algorithms based on randomized trees: **Random Forest** and **Extra randomness** algorithms.\n",
    "\n",
    "- **RandomForestClassifier** and **RandomForestRegressor** classes are used to deal with random forests.\n",
    "    - In random forests, each estimator is built from a sample drawn with replacement from the training set.\n",
    "- **ExtraTreesClassifier** and **ExtraTreesRegressor** classes are used to deal with extremely randomized forests.\n",
    "    - In extremely randomized forests, more randomness is introduced, which further reduces the variance of the model.\n",
    "\n",
    "#### 3. Boosting Methods\n",
    "**Boosting Methods** combine several weak models to create a improvised ensemble.\n",
    "\n",
    "- **sklearn.ensemble** also provides the following boosting algorithms:\n",
    "    - AdaBoostClassifier\n",
    "    - GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Train Data : 0.9906103286384976\n",
      "Accuracy of Test Data : 0.958041958041958\n"
     ]
    }
   ],
   "source": [
    "# Demo of Random Forest Classifier.\n",
    "# max depth can be changed to see effect on accuracy.\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(max_depth=4,)\n",
    "rf_classifier = rf_classifier.fit(X_train, Y_train) \n",
    "\n",
    "print('Accuracy of Train Data :', rf_classifier.score(X_train,Y_train))\n",
    "print('Accuracy of Test Data :', rf_classifier.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "**Support Vector Machines** (SVMs) separates data points based on **decision planes**, which separates objects belonging to **different classes** in a **higher dimensional space**.  \n",
    "\n",
    "SVM algorithm uses the best suitable kernel, which is capable of separating data points into **two or more classes**.\n",
    "Commonly used kernels are:  \n",
    "- linear\n",
    "- polynomial\n",
    "- rbf\n",
    "- sigmoid\n",
    "\n",
    "### Support Vector Regression\n",
    "**scikit-learn provides** the following three utilities for performing Support Vector Regression.\n",
    "- SVR\n",
    "- NuSVR\n",
    "- LinearSVR\n",
    "\n",
    "### Advantages of SVMs\n",
    "- SVM can distinguish the classes in a higher dimensional space.\n",
    "- SVM algorithms are memory efficient.\n",
    "- SVMs are versatile, and a different kernel can be used by a decision function.\n",
    "\n",
    "### Disadvantages of SVMs\n",
    "- SVMs do not perform well on high dimensional data with many samples.\n",
    "- SVMs work better only with Preprocessed data.\n",
    "- They are harder to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Train Data : 0.9178403755868545\n",
      "Accuracy of Test Data : 0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "# Demo of Support Vector Classification\n",
    "# The shown model overfits the training data.\n",
    "\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "svm_classifier = SVC()\n",
    "svm_classifier = svm_classifier.fit(X_train, Y_train)\n",
    "\n",
    "print('Accuracy of Train Data :', svm_classifier.score(X_train,Y_train))\n",
    "print('Accuracy of Test Data :', svm_classifier.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Train Data : 0.9788732394366197\n",
      "Accuracy of Test Data : 0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "# Let's finetune the model and improve accuracy using scaled data\n",
    "\n",
    "cancer = datasets.load_breast_cancer()  # Loading the data set\n",
    "\n",
    "standardizer = preprocessing.StandardScaler()\n",
    "standardizer = standardizer.fit(cancer.data)\n",
    "cancer_standardized = standardizer.transform(cancer.data)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    cancer_standardized, \n",
    "    cancer.target,\n",
    "    stratify=cancer.target,\n",
    "    random_state=42)\n",
    "\n",
    "svm_classifier = SVC()\n",
    "\n",
    "svm_classifier = svm_classifier.fit(X_train, Y_train)\n",
    "svm_classifier\n",
    "print('Accuracy of Train Data :', svm_classifier.score(X_train,Y_train))\n",
    "print('Accuracy of Test Data :', svm_classifier.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Viewing the Classification Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97        53\n",
      "           1       0.99      0.98      0.98        90\n",
      "\n",
      "    accuracy                           0.98       143\n",
      "   macro avg       0.98      0.98      0.98       143\n",
      "weighted avg       0.98      0.98      0.98       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "Y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "print('Classification report : \\n',\n",
    "      metrics.classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "## Clustering\n",
    "*Clustering is one of the unsupervised learning technique.*\n",
    "\n",
    "- The technique is typically used to group data points into clusters based on a specific algorithm.\n",
    "- Major clustering algorithms that can be implemented using scikit-learn are:\n",
    "\n",
    "   - K-means Clustering\n",
    "   - Agglomerative clustering\n",
    "   - DBSCAN clustering\n",
    "   - Mean-shift clustering\n",
    "   - Affinity propagation\n",
    "   - Spectral clustering\n",
    "   \n",
    "### K-Means Clustering\n",
    "*In K-means Clustering entire data set is grouped into k clusters.*\n",
    "\n",
    "- Steps involved are:\n",
    "    - k centroids are chosen randomly.\n",
    "    - The distance of each data point from k centroids is calculated. A data point is assigned to the nearest cluster.\n",
    "    - Centroids of k clusters are recomputed.\n",
    "    - The above steps are iterated till the number of data points a cluster reach convergence.\n",
    "- **KMeans** from **sklearn.cluster** can be used for K-means clustering.\n",
    "\n",
    "### Agglomerative Hierarchical Clustering\n",
    "*Agglomerative Hierarchical Clustering is a bottom-up approach.*\n",
    "\n",
    "- Steps involved are:\n",
    "    - Each data point is treated as a single cluster at the beginning.\n",
    "    - The distance between each cluster is computed, and the two nearest clusters are merged together.\n",
    "    - The above step is iterated till a single cluster is formed.\n",
    "    - **AgglomerativeClustering** from **sklearn.cluster** can be used for achieving this.\n",
    "    - Merging of two clusters can be any of the following **linkage** type: **ward, complete or average**.\n",
    "    \n",
    "### Density Based Clustering\n",
    "Now let's understand how density-based clustering is performed. DBSCAN from sklearn.cluster is used for this purpose.\n",
    "video https://www.coursera.org/lecture/machine-learning-with-python/dbscan-B8ctK\n",
    "\n",
    "\n",
    "### Mean Shift Clustering\n",
    "*Mean Shift Clustering aims at discovering dense areas.*\n",
    "\n",
    "Steps Involved:\n",
    "- Identify blob areas with randomly guessed centroids.\n",
    "- Calculate the centroid of each blob area and shift to a new one, if there is a difference.\n",
    "- Repeat the above step till the centroids converge.\n",
    "**make_blobs** from **sklearn.cluster** can be used to initialize the blob areas. **MeanShift** from **sklearn.cluster** can be used to perform Mean Shift clustering.\n",
    "\n",
    "### Affinity Propagation\n",
    "*Affinity Propagation generates clusters by passing messages between pairs of data points, until convergence.*\n",
    "\n",
    "- **AffinityPropagation** class from **sklearn.cluster** can be used.\n",
    "- The above class can be controlled with two major parameters:\n",
    "    - **preference**: It controls the number of exemplars to be chosen by the algorithm.\n",
    "    - **damping**: It controls numerical oscillations while updating messages.\n",
    "\n",
    "### Spectral Clustering\n",
    "*Spectral Clustering is ideal to cluster data that is connected, and may not be in a compact space.*\n",
    "\n",
    "In general, the following steps are followed:\n",
    "- Build an affinity matrix of data points.\n",
    "- Embed data points in a lower dimensional space.\n",
    "- Use a clustering method like k-means to partition the points on lower dimensional space.\n",
    "\n",
    "**spectral_clustering** from **sklearn.cluster** can be used for achieving this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo of KMeans\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans_cluster = KMeans(n_clusters=2)\n",
    "kmeans_cluster = kmeans_cluster.fit(X_train) \n",
    "kmeans_cluster.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a Clustering algorithm\n",
    "*A clustering algorithm is majorly evaluated using the following scores:*\n",
    "\n",
    "**Homogeneity**: Evaluates if each cluster contains only members of a single class.  \n",
    "**Completeness**: All members of a given class are assigned to the same cluster.  \n",
    "**V-measure**: Harmonic mean of Homogeneity and Completeness.  \n",
    "**Adjusted Rand index**: Measures similarity of two assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5272254837937459\n",
      "0.5140247069822967\n",
      "0.5205414168879022\n",
      "0.6422411377783216\n"
     ]
    }
   ],
   "source": [
    "# from sklearn import metrics\n",
    "\n",
    "print(metrics.homogeneity_score(kmeans_cluster.predict(X_test), Y_test))\n",
    "print(metrics.completeness_score(kmeans_cluster.predict(X_test), Y_test))\n",
    "print(metrics.v_measure_score(kmeans_cluster.predict(X_test), Y_test))\n",
    "print(metrics.adjusted_rand_score(kmeans_cluster.predict(X_test), Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "type(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.8 ns ± 0.275 ns per loop (mean ± std. dev. of 7 runs, 100000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "2+3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
